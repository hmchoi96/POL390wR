top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill=sentiment)) +
geom_col(show.legend = F) +
facet_wrap(~sentiment, scales="free_y") +
labs(x= "contribution to Sentiment", y=NULL)
library(reshape2)
library(wordcloud)
# Compare positive & negative words in the word cloud
leviathan_tibble %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = T) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"), max.words = 30)
# Tokenizing by n-gram / n=2 -> bigram
republic_bigrams <- gutenberg_download(150) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
# Counting and filtering n-grams
republic_bigrams %>% count(bigram, sort = TRUE)
# Can see a lot are stop words. Remove them:
library(tidyr)
bigrams_separated <- republic_bigrams %>%   separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%   # if word1/2 is not in stop_word, don't filter
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%    count(word1, word2, sort = TRUE)
# tidyr’s unite() function is the opposite of separate(), and lets us recombine the columns into one. Thus, “separate/filter/count/unite” let us find the most common bigrams not containing stop-words
bigrams_united <- bigrams_filtered %>%
# filter(vignettes == "2")
unite(bigram, word1, word2, sep = " ")
# Analyzing bigrams
bigrams_separated %>%
filter(word2 == "not") %>%           # find second word is truth
count(gutenberg_id, word1, sort = T)
# Code here
leviathan_bigrams <- gutenberg_download(3702) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
leviathan_bigrams %>%
count(bigram, sort = TRUE)
bigrams_separated <- leviathan_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>%    count(word1, word2, sort = TRUE)
bigrams_separated %>%
filter(word1 == "not") %>%
count(gutenberg_id, word1, sort = TRUE)
AFINN <- get_sentiments("afinn")
library(ggplot2)
not_words <- bigrams_separated %>%
filter(word1 == "not") %>%
filter(gutenberg_id == "150") %>%
inner_join(AFINN, join_by(word2 == "word")) %>%
count(word2, value, sort = T)
not_words %>%
mutate(contribution = n*value) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(n * value, word2, fill = n * value >0)) +
geom_col(show.legend = F)
#+ labs(x = "Sentiment value * number of occurances", y = "Words preceded by not")
library(dplyr)
library(stringr)
library(tidyr)
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
republic_tibble %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
# Graphing the top joy words
library(ggplot2)
republic_joy_plot <- republic_tibble %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>% # Plot the count
filter(n > 30) %>% # Only words used 50 times or more
mutate(word = reorder(word, n)) %>% # Sorted by n
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
AFINN <- get_sentiments("afinn")
library(ggplot2)
not_words <- bigrams_separated %>%
filter(word1 == "not") %>%
filter(gutenberg_id == "150") %>%
inner_join(AFINN, join_by(word2 == "word")) %>%
count(word2, value, sort = T)
not_words %>%
mutate(contribution = n*value) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(n * value, word2, fill = n * value >0)) +
geom_col(show.legend = F)
#+ labs(x = "Sentiment value * number of occurances", y = "Words preceded by not")
rm(list = ls())
## books with expired copyright can be brought
library(gutenbergr)
# Example of string data (from Machiavelli's The Prince)
text <- c("Every one sees what you appear to be, few really know what you are, and those few dare not oppose themselves to the opinion of the many, who have the majesty of the state to defend them...",
"For that reason, let a prince have the credit of conquering and holding his state, the means will always be considered honest, and he will be praised by everybody;",
"because the vulgar are always taken by what a thing seems to be and by what comes of it")
# To turn it into a tidy text dataset, must put it into a data frame:
library(dplyr)
text_df <- tibble(line = 1:3, text = text)
# convert this to one-token-per-document-per-row
library(tidytext)
text_df %>%
unnest_tokens(word, text)
# Load the data for the first book:
rm(list = ls())
gutenberg_works(author == "Plato")
republic_tibble <- gutenberg_download(150)
# Packages for text mining/ Word cloud stuff
library(tidytext)
library(stringr)
library(dplyr)
library(rtweet)
library(wordcloud2)
library(corpustools)
library(SnowballC)
library(tm)
stopwords("English")
# Remove numbers
republic_tibble <- republic_tibble[-grep("\\b\\d+\\b",
republic_tibble$text),]
# Creating a tibble
# regex -> remove puncuation
republic_tibble <- republic_tibble %>%
unnest_tokens(word, text, token = "regex",
pattern = "\\s+|[[:punct:]]+") %>%
anti_join(stop_words) %>%
mutate(stem = wordStem(word))
# Word frequency plot
library(ggplot2)
word_frequency_republic_plot <- republic_tibble %>%
count(word, sort = T) %>%
filter(n > 100) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
# Stem frequency plot
stem_frequency_republic_plot <- republic_tibble %>%
count(stem, sort = T) %>%
filter(n > 100) %>%
mutate(stem = reorder(stem, n)) %>%
ggplot(aes(n, stem)) +
geom_col() +
labs(y = NULL)
# Stem is useful for more precise analysis but harder to convey meaning to audiences
# Wordcloud with words
# Create a word count
word_count_republic <- republic_tibble %>%
count(word, sort = T)
# Create word cloud:
set.seed(6037)
wordcloud2(word_count_republic, size = 1)
# Wordcloud with words
# Create a stem count
word_count_republic
library(rmarkdown)
rm(list = ls())
## books with expired copyright can be brought
library(gutenbergr)
# Example of string data (from Machiavelli's The Prince)
text <- c("Every one sees what you appear to be, few really know what you are, and those few dare not oppose themselves to the opinion of the many, who have the majesty of the state to defend them...",
"For that reason, let a prince have the credit of conquering and holding his state, the means will always be considered honest, and he will be praised by everybody;",
"because the vulgar are always taken by what a thing seems to be and by what comes of it")
# To turn it into a tidy text dataset, must put it into a data frame:
library(dplyr)
text_df <- tibble(line = 1:3, text = text)
# convert this to one-token-per-document-per-row
library(tidytext)
text_df %>%
unnest_tokens(word, text)
# Load the data for the first book:
rm(list = ls())
gutenberg_works(author == "Plato")
republic_tibble <- gutenberg_download(150)
# Packages for text mining/ Word cloud stuff
library(tidytext)
library(stringr)
library(dplyr)
library(rtweet)
library(wordcloud2)
library(corpustools)
library(SnowballC)
library(tm)
stopwords("English")
# Remove numbers
republic_tibble <- republic_tibble[-grep("\\b\\d+\\b",
republic_tibble$text),]
# Creating a tibble
# regex -> remove puncuation
republic_tibble <- republic_tibble %>%
unnest_tokens(word, text, token = "regex",
pattern = "\\s+|[[:punct:]]+") %>%
anti_join(stop_words) %>%
mutate(stem = wordStem(word))
# Word frequency plot
library(ggplot2)
word_frequency_republic_plot <- republic_tibble %>%
count(word, sort = T) %>%
filter(n > 100) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
# Stem frequency plot
stem_frequency_republic_plot <- republic_tibble %>%
count(stem, sort = T) %>%
filter(n > 100) %>%
mutate(stem = reorder(stem, n)) %>%
ggplot(aes(n, stem)) +
geom_col() +
labs(y = NULL)
# Stem is useful for more precise analysis but harder to convey meaning to audiences
# Wordcloud with words
# Create a word count
word_count_republic <- republic_tibble %>%
count(word, sort = T)
# Create word cloud:
set.seed(6037)
wordcloud2(word_count_republic, size = 1)
# Wordcloud with words
# Create a stem count
word_count_republic
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
runApp('~/study_R/POL390/shiny/sss')
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig)
# Answer here
wages <- Wage$wage
wageMean <- mean(wages)
wageSD <- (sd(wages-wageMean)/sqrt(3000))
observedMean <- 110
z_score <- (observedMean - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig)
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig)
# Answer here
wages <- Wage$wage
wageMean <- mean(wages)
wageSD <- (sd(wages-wageMean)/sqrt(3000))
observedMean <- 110
z_score <- (observedMean - wageMean) / wageSD
probBig <- 1- pnorm((z_score))
print(probBig)
# Answer here
wages <- Wage$wage
wageMean <- mean(wages)
wageSD <- (sd(wages-wageMean)/sqrt(3000))
observedMean <- 110
z_score <- (observedMean - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig)
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig, "reject")
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig, "reject")
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig + "reject")
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig)
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score)
print(probBig)
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score))
print(probBig)
# Answer here
wages <- Wage$wage
wageMean <- mean(wages)
wageSD <- (sd(wages-wageMean)/sqrt(3000))
observedMean <- 110
z_score <- (observedMean - wageMean) / wageSD
probBig <- 1- pnorm((z_score))
print(probBig)
# Answer here
wages <- Wage$wage
wageMean <- mean(wages)
wageSD <- (sd(wages-wageMean)/sqrt(3000))
observedMean <- 110
z_score <- (observedMean - wageMean) / wageSD
probBig <- 1- pnorm((z_score), lower.tail = F)
print(probBig)
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score))
print(probBig)
# Answer here
meanH0 <- 110
z_score <- (meanH0 - wageMean) / wageSD
probBig <- 1- pnorm((z_score))
print(probBig)
print("do not reject")
# Answer here
margin_of_error <- 1.96 * (wageSD / sqrt(3000))
confidence_interval <- c(wageMean - margin_of_error, wageMean + margin_of_error)
confidence_interval
# Answer here
margin_of_error <- 1.96 * (wageSD / sqrt(3000))
confidence_interval <- c(wageMean - margin_of_error, wageMean + margin_of_error)
print(confidence_interval)
# Answer here
view(Wage)
glimpse(Wage)
summary(Wage)
indsWageMean <- mean(Wage[Wage$jobclass=="Industrial"])
indsWageMean
indsWageMean <- mean(Wage$wage[Wage$jobclass=="Industrial"])
indsWageMean <- mean(Wage$wage[Wage$jobclass=="Industrial"])
indsWageMean
# Answer here
summary(Wage)
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
wbar_industrial
w_bar_var_industrial
wbar_information
w_bar_var_information
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
wbar_industrial
w_bar_var_industrial
wbar_information
w_bar_var_information
# install.packages("ISLR")
library(ISLR)
data(Wage)
summary(Wage)
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
wbar_industrial
w_bar_var_industrial
wbar_information
w_bar_var_information
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
wbar_industrial
w_bar_var_industrial
wbar_information
w_bar_var_information
# Answer here
# Perform two-sample t-test
t_test_result <- t.test(industrial_data$wage, information_data$wage)
# Answer here
# Perform two-sample t-test
t_test_result <- t.test(industrial_data$wage, information_data$wage)
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
wbar_industrial
w_bar_var_industrial
wbar_information
w_bar_var_information
industrial_data
# Answer here
summary(Wage)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "Industrial")
industrial_data
Wage
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "1. Industrial")
industrial_data
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "2. Information")
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "1. Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "2. Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
wbar_industrial
w_bar_var_industrial
wbar_information
w_bar_var_information
w_bar_var_industrial
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "1. Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "2. Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
wbar_industrial
w_bar_var_industrial
wbar_information
w_bar_var_information
# Answer here
summary(Wage)
library(dplyr)
# Filter data for Industrial jobs
industrial_data <- filter(Wage, jobclass == "1. Industrial")
wbar_industrial <- mean(industrial_data$wage)
w_bar_var_industrial <- var(industrial_data$wage)
# Filter data for Information jobs
information_data <- filter(Wage, jobclass == "2. Information")
wbar_information <- mean(information_data$wage)
w_bar_var_information <- var(information_data$wage)
# Display the results
print(wbar_industrial)
print(w_bar_var_industrial)
print(wbar_information)
print(w_bar_var_information)
# Answer here
# Perform two-sample t-test
t_test_result <- t.test(industrial_data$wage, information_data$wage)
# Extract the p-value
p_value <- t_test_result$p.value
# Display the p-value
p_value
# Answer here
t_test_result <- t.test(industrial_data$wage, information_data$wage)
p_value <- t_test_result$p.value
print(p_value)
print("Do not reject")
